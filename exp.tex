\chapter{Experimental Results\label{chap:exp}}
The overhead introduced by oxc framework composes three parts:
\begin{itemize}
\item The degression in modular schedulers' performance when they 
	work under the oxc framework. This is largly because current
	framework implementation is not matured enough.
\item The time required to execute codes brought framework functions.
\item The context switches introduced by the oxc framework.
\end{itemize}

Current oxc framework implementation is still a prototype one. Some kernel 
features are not considered under the framework yet. For example, the 
\texttt{priority inheritance}, which is important for the kernel's real time
performance and will influence number of context switches. 
However, the execution time of oxc function codes is fixed and can be measured.
As for the context switches caused by importng CBS based scheduling in the
kernel, there is previous work .... This may help readers understand what 
happens under the oxc framework. In the rest of this chapter, our experiment
will concentrate on measuring execution time of oxc functions.

In a Linux system, even if the oxc patch is applied in the kernel, when there 
is no reservation enabled, the system performs as a plain Linux system. 
The possible overheads in this case include the code execution time in 
function \texttt{is\_oxc\_task} and the oxc related initialization when a 
scheduling group is created; both are negligible.

The hardware and software used in the experiment is shown in 
table \ref{tab:exp_setup}.
\begin{table}[thbp]
  \centering
  \begin{tabular}{ll}\hline
	\emph{Hardware platform}\hspace{4cm}		& 	\\
	Processor			& Intel(R) Core(TM) Duo E8500	 \\
	Frequency			& 3.16GHz\\
					&	\\	
	\emph{Software platform}\hspace{4cm}		& 	\\
	Linux distribution		& Ubuntu 11.10\\
	Compiler version		& gcc 4.6.1\\
	Kernel version			& 3.4.0-rc+ \\\hline
  \end{tabular}
  \caption{Hardware-Software platform}
  \label{tab:exp_setup}
\end{table}
\section{Ftrace in Linux kernel}
Ftrace is an internal tracer designed to help out developers of systems to
find out what is going on inside the kernel. The name ftrace comes from
''function tracer'', which is its original purpose and the reason it is 
used here. Now there are various kinds of tracers incorporated in Ftrace.
You can use it to trace context switces, hong long interrupts are disabled,
and so on.

Ftrace uses \emph{debugfs} file system to hold control files as well as
file to display output. 
Typically, ftrace is mounted at \texttt{/sys/kernel/debug}.
\begin{lstlisting}
	#mount -t debugfs nodev /sys/kernel/debug
\end{lstlisting}
After this command, a firectory \texttt{/sys/kernel/debug/tracing} will 
be created containing interfaces to configure ftrace and display results.
\begin{lstlisting}
	#cd /sys/kernel/debug/tracing
\end{lstlisting}
The following commands will be assumed to be called under \texttt{tracing}
directory.
There are several kinds of tracers available in ftrace, simply cat the
\texttt{available\_tracers} file in the \texttt{tracing} dorectory.
\begin{lstlisting}
	#cat available_tracers
	blk function_graph mmiotrace wakeup_rt wakeup function sched_switch nop
\end{lstlisting}
The \texttt{function} is function tracer. It uses the \texttt{-pg} option
of \texttt{gcc} to have every function in the kernel call a special function
\texttt{mcount()} for tracing all kernel functions and measure execution time 
of them.  This is what we need. To enable the function tracer, just \emph{echo} \texttt{function} into the \texttt{current\_tracer}
file.
\begin{lstlisting}
	#echo function > current_tracer
\end{lstlisting}
The trace can be started and stopped through configuring \texttt{tracing\_on}
file. Echo 0 into this file to disable the tracer or 1 to enable it. Cat the
file will displat whether the tracer is enabled or not.

The output of the trace in held in file \texttt{trace} in a human readable
format. The ftrace will by default trace all functions in the kernel. In
most cases, people only care about particular functions. To dynamically
configure which function to trace, the \texttt{CONFIG\_DYNAMIC\_FTRACE}
kernel option should be set in compilation time  to enable dynamic ftrace. 
Actually, \texttt{CONFIG\_DYNAMIC\_FTRACE} is highly recommanded and defaultly
set because of its performance enhancement. To filter which function to trace
or not, two files are used, one for enabling and one for disabling the 
tracing of specific functions. They are \texttt{set\_ftrace\_filter} and 
\texttt{set\_ftrace\_notrace}. A list of available functions that you can add
to these files is listed in \texttt{available\_filter\_functions}.

\section{Dbench}
The dbench benchmark is a tool that measures disk throughput for simulated
netbench run Dbench reads a load description file called client.txt that was
derived from a network sniffer dump of a real netbench run and produces the
filesystem load according to the description file. It does no networking calls.
One exmple to run dbench test:
\begin{lstlisting}
	dbench 2 -t 100
\end{lstlisting}
\begin{itemize}
\item 2 indicates the number of clients in the simulated netbench run.
\item \texttt{-t} sets the runtime of the benchmark in seconds (default 600).
\end{itemize}
So, this command means that two dbench threads will run simultaneously for 
100 seconds.

\section{Overhead evaluation}
\subsection{The experiment design}
There will be six individual tests differing in the number of
hyper ox containers in the system, from 1 to 6.
The hyper ox containers in the experiment are identical.
Each hyper ox-container has two ox containers with CPU reservation parameter
$0.1ms/1ms$. Each ox container has one dummy task within it. The dummy task
is simply running a forever while loop and it is a normal task. This while loop
task will exhuaust the reservation in a ox container. During each 
test, the execution time of following oxc functions are measured by Ftrace:
\begin{itemize} 
\item \texttt{check\_preempt\_curr\_oxc}
\item \texttt{pick\_next\_task\_oxc}
\item \texttt{put\_prev\_task\_oxc}
\item \texttt{set\_curr\_task\_oxc}
\item \texttt{task\_tick\_oxc}
\end{itemize} 
There are other oxc functions, like \texttt{enqueue\_task\_rq\_oxc}, not
included in the list because they are not called so often.

The measured execution time of above oxc functions comprises the time 
consumed by codes involving with the oxc control and operations defined 
in modular scheduler which are encapsulated inside the above functions.
Here the situation for cfs scheduling inside an ox container is very simple.
And the experiment analysis can focus on oxc control overhead.

\subsection{Experiment results}





